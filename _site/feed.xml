<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-24T21:01:36+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SabotageAndi Blog</title><subtitle>Personal blog migrated from Hugo to Jekyll</subtitle><entry><title type="html">From Need to Published NuGet Package in Less Than 8 Hours</title><link href="http://localhost:4000/2019/02/17/from-need-to-published-nuget-package-less-than-8-hours.html" rel="alternate" type="text/html" title="From Need to Published NuGet Package in Less Than 8 Hours" /><published>2019-02-17T14:18:59+01:00</published><updated>2019-02-17T14:18:59+01:00</updated><id>http://localhost:4000/2019/02/17/from-need-to-published-nuget-package-less-than-8-hours</id><content type="html" xml:base="http://localhost:4000/2019/02/17/from-need-to-published-nuget-package-less-than-8-hours.html"><![CDATA[I was trying to get SpecFlow to build under Mac OS and had some problems with an MSBuild target (<https://github.com/techtalk/SpecFlow/blob/master/Tests/TechTalk.SpecFlow.Specs/.build/build.targets#L32>).  
We need to replace some text in a file with a value which we only can access during build. For that we used the [RoslynCodeTaskFactory](https://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild-roslyncodetaskfactory?view=vs-2017). This has the benefit, of not having to write a whole MSBuild task. But I had problems that `msbuild` or `dotnet build` wasn't able to find the `Microsoft.Build.Tasks.Core.dll`.  
So I decided to write a new MSBuild task anyway.  
But this post is not about writing the n-th MSBuild task, but about that I was able to write it, create a NuGet package for it, have a continues build with automatic deployment to NuGet.org. And I was able to do it in less than 8 hours (and on a Mac).

You can find the whole sources [here](https://github.com/SabotageAndi/MSBuild.AdditionalTasks).

## Writing the MSBuild task

Writing a MSBuild task is not hard. You need to reference `Microsoft.Build.Framework` and `Microsoft.Build.Utilities.Core` in your project. In your class you have to implement the `Task` abstract class <https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/src/MSBuild.AdditionalTasks/Tasks/ReplaceTextInFile/ReplaceTextInFileTask.cs#L8>. In the `Execute`- method you implement your task.

And done!

## Creating a NuGet package for the MSBuild task to redistribute it

Thanks to the sdk- style project system, for most of the time when you create a NuGet package, a `nuspec`- file is not needed anymore. Everything can be specified in the project file.
Packaging a MSBuild task is a little bit different than normal libraries. You have to put the assembly into the `tasks` folder and not in the standard `lib` folder. And then you need additional MSBuild files to register your task into MSBuild (<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/tree/master/src/MSBuild.AdditionalTasks/build>). Thanks to Nate McMaster who wrote a blog post about packaging MSBuild tasks (<https://natemcmaster.com/blog/2017/07/05/msbuild-task-in-nuget/>).

What has changed since he published his article, is that NuGet.org now wants a license included in the package. For that you need to specify in which file the license is (<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/src/MSBuild.AdditionalTasks/MSBuild.AdditionalTasks.csproj#L22>) and package the license file into your package (<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/src/MSBuild.AdditionalTasks/MSBuild.AdditionalTasks.csproj#L35>).

To make it easier in the build pipeline to put the generated NuGet package into the build artifacts, I configured the output folder for packages to be outside of the project (<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/src/MSBuild.AdditionalTasks/MSBuild.AdditionalTasks.csproj#L31>).
This makes it also easier to use the generated package in a test/sample project.

Additionally I enabled `GeneratePackageOnBuild` so that the NuGet package is generated everytime the project is build (<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/src/MSBuild.AdditionalTasks/MSBuild.AdditionalTasks.csproj#L30>).

To check if everything was correct in the NuGet package, I uploaded it manually to NuGet.org. That's were I got the notice that I have to add the license into the package. ;-)

## Continue Build

I love Azure Pipelines. And since they provide 10 free unlimited hosted build agents for Open Source project (<https://azure.microsoft.com/en-us/blog/announcing-azure-pipelines-with-unlimited-ci-cd-minutes-for-open-source/>), it's better than before (limit to 300 min per month). And they have hosted build agents for Windows and Mac OS.

You can use the web UI to define your build pipeline or you can do it per yaml (<https://docs.microsoft.com/en-us/azure/devops/pipelines/get-started-yaml?view=azure-devops>).
Yaml has the benefit, that you can define the same pipeline and run it as jobs on different agent pools.

Azure Pipeline definition: <https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/azure-pipelines.yml>
Job definition: <https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/build.yml>

Build status: https://sabotageandi.visualstudio.com/MSBuild.AdditionalTasks/_build?definitionId=6

The part where I struggled the most was to find the right path for the `PublishBuildArtifact` task (<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/build.yml#L58>). At the end the problem was, that I specified a file pattern. Looks like that doesn't work with the task.

## Deploy it to NuGet.org

With now a working build pipeline, I created a new release pipeline that takes the artifact and uploads the nupkgs- files to NuGet.org.
As of writing this article, only the UI is available to define release pipelines.

### Release pipeline

![release pipeline](/images/2019-02-17%20Release%20Pipeline.png)

### Release pipeline detail

![release pipeline detail](/images/2019-02-17%20Release%20Pipeline%20details.png)

The "hardest" part was to create the service connection with NuGet.org. For that, you need an API key for your account. The steps for that are documented [here](https://docs.microsoft.com/en-us/nuget/create-packages/publish-a-package#create-api-keys).

When you have it, you can create a NuGet service connection in Azure Pipelines.

![nuget service connection](/images/2019-02-17%20NuGet%20Service%20Connections.png)

This service reference has to be used in the NuGet task. And after specifing the correct path where the NuGet packages are (yes, again pathes are hard).

And that's it. At the second run of the release pipeline, a new version of NuGet package was uploaded to NuGet.org. Yeah!

## Problems on the way

### Building on different platforms

I wanted to have one way to build the task on Windows and Mac OS. I choose PowerShell, because it is available on Mac OS and Windows.
To build I used `dotnet build` and so I don't have to find `msbuild` on my system (MacOS: in PATH, Windows: search via vswhere.exe). But because I build the sample/test in the same script, the NuGet- package gets cached in the local package cache. So I need to delete the cached files before the build.

Obvious dependend on the OS, there are different ways to get the folder. In this case, a simple check if an environment variable exists ($HOME for Mac OS)
<https://github.com/SabotageAndi/MSBuild.AdditionalTasks/blob/master/build_and_tests.ps1#L7>.

As we got the folder, removing it is easy, if you know PowerShell (I didn't, now I do a little bit more).

## Conclusion

It's facinating how easy this stuff got in the last years. Some years ago doing this all was work of multiple days. Now I was able to do it in some hours. And the most part of the time I spend on the build PowerShell script.

So I think I should learn PowerShell. :-)]]></content><author><name></name></author><summary type="html"><![CDATA[I was trying to get SpecFlow to build under Mac OS and had some problems with an MSBuild target (https://github.com/techtalk/SpecFlow/blob/master/Tests/TechTalk.SpecFlow.Specs/.build/build.targets#L32). We need to replace some text in a file with a value which we only can access during build. For that we used the RoslynCodeTaskFactory. This has the benefit, of not having to write a whole MSBuild task. But I had problems that msbuild or dotnet build wasn’t able to find the Microsoft.Build.Tasks.Core.dll. So I decided to write a new MSBuild task anyway. But this post is not about writing the n-th MSBuild task, but about that I was able to write it, create a NuGet package for it, have a continues build with automatic deployment to NuGet.org. And I was able to do it in less than 8 hours (and on a Mac).]]></summary></entry><entry><title type="html">Comparision of Cross- Platform UI Libraries for .NET</title><link href="http://localhost:4000/2019/02/13/cross-platform-ui-libraries.html" rel="alternate" type="text/html" title="Comparision of Cross- Platform UI Libraries for .NET" /><published>2019-02-13T14:33:00+01:00</published><updated>2019-02-13T14:33:00+01:00</updated><id>http://localhost:4000/2019/02/13/cross-platform-ui-libraries</id><content type="html" xml:base="http://localhost:4000/2019/02/13/cross-platform-ui-libraries.html"><![CDATA[For an upcoming spare time project I need a UI library that work cross- plattform (Windows, Mac OS and Linux). So I began to research research what possibilities are currently available.

I found following libraries:

- [Xamarin.Forms](https://docs.microsoft.com/en-us/xamarin/xamarin-forms/)
- [Uno Platform](https://platform.uno/)
- [Avalonia UI](http://avaloniaui.net/)

Let's have a look at some general data of them.

|                    | Xamarin.Forms                        | Uno Platform | Avalonia UI |
|---|---|---|---|
| __Supported platforms__ | Androis, iOS, Windows, Mac OS, Linux | Android, iOS, Windows, WebAssembly | Windows, Mac OS, Linux |
...existing code...]]></content><author><name></name></author><summary type="html"><![CDATA[For an upcoming spare time project I need a UI library that work cross- plattform (Windows, Mac OS and Linux). So I began to research research what possibilities are currently available.]]></summary></entry><entry><title type="html">The Road to .NET Core support in SpecFlow</title><link href="http://localhost:4000/2019/01/15/road-to-dotnet-core-support-specflow.html" rel="alternate" type="text/html" title="The Road to .NET Core support in SpecFlow" /><published>2019-01-15T15:11:06+01:00</published><updated>2019-01-15T15:11:06+01:00</updated><id>http://localhost:4000/2019/01/15/road-to-dotnet-core-support-specflow</id><content type="html" xml:base="http://localhost:4000/2019/01/15/road-to-dotnet-core-support-specflow.html"><![CDATA[This blog post is about the history how we got .NET Core support into SpecFlow and what we learned from that.

We are in this blog post my team at TechTalk, who worked with me to achieve this.  
These are:  

- David Eiwen
- Nesli Sanli
- Balázs Epresi
- Stephen McCafferty

Around a year ago, we made an estimation how much it would cost to add .NET Core support to SpecFlow and SpecFlow+Runner.  
But the work for it began much earlier. The big PR for it (<https://github.com/techtalk/SpecFlow/pull/649>) was created on June 18th 2016.

So why the hell did we need 2 1/2 years to finish it?

First, SpecFlow is a Open Source project. We live as every other from the contributions to it. Sadly there weren't that many for it in the first 1 1/2 years.
But back to January 2018.  
After a lot of requests from the community David, Nesli and me did the estimation for it. After that TechTalk decided, that it will sponsor the development of it. So say thanks to them (or by some SpecFlow+ licenses). ;-)  
All three of us needed to finish our current projects to finally start with the .NET Core support.
And so we were at end of March.

# Some work was already done

Some work was already done in the 1 1/2 years ago. The biggest one was the migration of all projects to the sdk-style project format.  

# Let the work begin

First we had to rewrite our whole test suite. The reason was, that to test SpecFlow, we create .NET projects, compile them and let a test runner run. Integrationtests on a new level. The current implementation didn't support a lot of features (sdk-style projects was the biggest) we needed and it had no chance to be adapted for it. So we started with [SpecFlow.TestProjectGenerator](https://github.com/techtalk/SpecFlow.TestProjectGenerator). This is a reusable library (at least for us, we use it also in SpecFlow+), to create projects in different styles for different programming languages, compile them via MSBuild or `dotnet build` and then run the tests with the `vstest.console.exe`.  

At the same time, we worked on a new MSBuild task to generate the code-behind files of the feature files. As the generation of the code-behind files via Visual Studio Extension is making more and more headaches, our goal was, that you use always the MSBuild code-behind file generation if you are using .NET Core.  
Because we had at various places Assembly load problems, we wanted to go with an Out of process model. So the MSBuild task gets executed, starts an external process. The task communicates with this process, which does the whole work. At the end the external process is closed again.  
Yeah, that didn't work that great.  
We did get it to work, but it was way to fragile.  
So we moved everything back into the MSBuild task. (Which will bring us into another problem later <https://github.com/techtalk/SpecFlow/issues/1305>)]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post is about the history how we got .NET Core support into SpecFlow and what we learned from that.]]></summary></entry><entry><title type="html">Xamarin.Forms + F# + Paket</title><link href="http://localhost:4000/2018/06/03/xamarin-forms-fsharp-paket.html" rel="alternate" type="text/html" title="Xamarin.Forms + F# + Paket" /><published>2018-06-03T01:00:00+02:00</published><updated>2018-06-03T01:00:00+02:00</updated><id>http://localhost:4000/2018/06/03/xamarin-forms-fsharp-paket</id><content type="html" xml:base="http://localhost:4000/2018/06/03/xamarin-forms-fsharp-paket.html"><![CDATA[Yesterday I started a new Xamarin.Forms project in F# on my Mac. Because the NuGet experience isn’t the best in Visual Studio for Mac, I always use Paket for managing my dependencies.

Normally I am always following the steps [here](https://fsprojects.github.io/Paket/paket-and-dotnet-cli.html) and [here](https://fsprojects.github.io/Paket/convert-from-nuget-tutorial.html). But this time, after I have done everything, the project didn’t compile anymore.

After some time of fiddling with Paket (I am not the expert user of it) I found the solution. You have to add a framework restriction to the Xamarin.Forms and Xamarin.Android.FSharp.ResourceProvider entry.

The reason is, that if you not add it, on Android Paket is using MonoAndroid1.0 as target framework and so the dependencies are resolved not correctly. I found this GitHub issue for this behaviour: <https://github.com/fsprojects/Paket/issues/2762>

The complete paket.dependencies file looks like this:

```
source https://www.nuget.org/api/v2
nuget FSharp.Core
nuget Xamarin.Forms framework: MonoAndroid8.1,netstandard2.0,xamarinios
nuget Xamarin.Android.FSharp.ResourceProvider framework:monoandroid8.1
```]]></content><author><name></name></author><summary type="html"><![CDATA[Yesterday I started a new Xamarin.Forms project in F# on my Mac. Because the NuGet experience isn’t the best in Visual Studio for Mac, I always use Paket for managing my dependencies.]]></summary></entry><entry><title type="html">How SpecFlow works – Overview</title><link href="http://localhost:4000/2018/03/07/how-specflow-works-overview.html" rel="alternate" type="text/html" title="How SpecFlow works – Overview" /><published>2018-03-07T00:00:00+01:00</published><updated>2018-03-07T00:00:00+01:00</updated><id>http://localhost:4000/2018/03/07/how-specflow-works-overview</id><content type="html" xml:base="http://localhost:4000/2018/03/07/how-specflow-works-overview.html"><![CDATA[In preparation of continuing the work on .NET Core/Standard support for SpecFlow and recognising in talks at the MVP Summit, that it is not that simple to explain how SpecFlow works, I thought it would be good to start writing it done.

So as starting ponts, let's have a look at the normal workflow, when you write a new Scenario?

1. The user opens or creates a feature file, writes it Scenario in Gherkin and saves.
2. A code-behind file is generated, which contains code for a test class of the configured unit test provider. This could be SpecFlow+Runner, xUnit, NUnit, MsTest or MbUnit.
3. The user selects the line and presses F12.
4. A dialog opens, which contains a skeleton for a binding method for this sentence.
5. The user adds the skeleton binding code to a new or existing binding class
6. The user implements the binding. Probably with some helper methods from the TechTalk.SpecFlow.Assists namespace.
7. The user compiles its project
8. The user runs the tests
9. The tests are hopefully green.

That's a lot of steps to get from a scenario to a executed tests.

And these are the parts that are involved in this:

- Generator
  - Code- Behind File generation
  - Plugins
- Runtime
  - Binding lookup
  - Assists
  - Plugins
- Visual Studio Extension
  - Syntax Highlighting/IntelliSense
  - Navigation
  - Code- Behind Generation
  - Skeleton Generation
  - ItemTemplates

More than you would think, or?

Additionally SpecFlow has also a console application (specflow.exe), which also contains crucial functionality:

- SpecFlow.exe
  - generateAll Command
  - MSBuild tasks
  - Reports

To make it easier to read for you (and to write for me), every major part will get it's own blog posts. So you don't have to wait until I am completely finished with everything. I will add links to the individual posts when they are ready.]]></content><author><name></name></author><summary type="html"><![CDATA[In preparation of continuing the work on .NET Core/Standard support for SpecFlow and recognising in talks at the MVP Summit, that it is not that simple to explain how SpecFlow works, I thought it would be good to start writing it done.]]></summary></entry><entry><title type="html">.NET Core Global Tools with UI</title><link href="http://localhost:4000/2018/03/04/dotnet-core-global-tools-with-ui.html" rel="alternate" type="text/html" title=".NET Core Global Tools with UI" /><published>2018-03-04T00:00:00+01:00</published><updated>2018-03-04T00:00:00+01:00</updated><id>http://localhost:4000/2018/03/04/dotnet-core-global-tools-with-ui</id><content type="html" xml:base="http://localhost:4000/2018/03/04/dotnet-core-global-tools-with-ui.html"><![CDATA[Sometimes after the .NET team announced that they will add global tools to .NET Core, I asked myself if it would be possible to combine them with [Avalonia](http://avaloniaui.net/) to make a global tool with an UI.

For those who don't know Avalonia, you should check it out. It's an UI framework that uses XAML like WPF and Xamarin Forms and is implemented for .NET Framework and .NET Core. It runs on Windows, Linux and Mac OS.

Back to the global tools.

TL;DR: You can use Avalonia together with global tools. Here is a Sample: <https://github.com/SabotageAndi/LicenseActivatorPrototype>

But what it's needed? Not much.

You have to add the PackAsTool and the ToolCommandName property.
PackAsTool marks the project as a tool for installing. ToolCommandName specify how the tool is called after installation.

This sets all, that you only have to do a dotnet pack to generate a NuGet package for the global tool.

After that, you can install it with a dotnet install tool -g Your_Tool_Name. If you have packages from additional sources than NuGet.org, you can add them via command line argument or have a NuGet.config. In my prototype I am using nightlies of Avalonia and so I need to configure their feed as an additional source. Additionally I configured a local source, where the generated NuGet package gets published. It is here: <https://github.com/SabotageAndi/LicenseActivatorPrototype/blob/master/NuGet.config>

I had one small problem, where I got an error during installation, that there would be a downgrade of the System.Net.Primitives packages happen and so the tool can not be installed. The fix was to reference the package in project directly.

So what does this mean, that we could use Avalonia in a global tool?

We are not limited to only command line global tools, but can also write rich UI applications. And we can simply deploy them with a simple command line call.

I would call this cool. Let's see what we will make out of it.

Thanks to Martin Ullrich (<https://dasmulli.blog/2018/01/23/exploring-global-net-core-tools/>) and Nate McMaster (<https://www.natemcmaster.com/blog/2018/02/02/dotnet-global-tool/>) for their blog posts about global tools.]]></content><author><name></name></author><summary type="html"><![CDATA[Sometimes after the .NET team announced that they will add global tools to .NET Core, I asked myself if it would be possible to combine them with Avalonia to make a global tool with an UI.]]></summary></entry><entry><title type="html">Where is my presenter mode?</title><link href="http://localhost:4000/2018/02/23/where-is-my-presenter-mode.html" rel="alternate" type="text/html" title="Where is my presenter mode?" /><published>2018-02-23T00:00:00+01:00</published><updated>2018-02-23T00:00:00+01:00</updated><id>http://localhost:4000/2018/02/23/where-is-my-presenter-mode</id><content type="html" xml:base="http://localhost:4000/2018/02/23/where-is-my-presenter-mode.html"><![CDATA[During my preparation for my latest meetup talk, I noticed that I wasn't able to switch to presenter mode in Visual Studio. The Quick launch tasks were simply not there.

What now? In the past they were part of the [productivity power tools](https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.ProductivityPowerPack2017) extension. This was installed, but no quick launch tasks for me. 🙁

After some research together with the audience, we found the reason. The presenter mode quick launch tasks were moved to a [separate extension](https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.QuickLaunchTasks) and this was not installed. The productivity power tools are now a meta extensions, that has dependencies on a lot of other extensions.

So after installing the extension and restarting VS I had the presenter mode back. So if you are searching it sometime in the future, the extension is called [Quick Launch Tasks](https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.QuickLaunchTasks).

PS: I am sure, that the extension was missing is due to my development setup, multiple dev machines (4 machines at 3 locations) and extension roaming. So no bad feeling against the extension developer for this.]]></content><author><name></name></author><summary type="html"><![CDATA[During my preparation for my latest meetup talk, I noticed that I wasn’t able to switch to presenter mode in Visual Studio. The Quick launch tasks were simply not there.]]></summary></entry><entry><title type="html">Blog post to my talk “MSBuild 101 – Was passiert wenn ich auf „Build“ klicke”</title><link href="http://localhost:4000/2018/02/04/blog-post-msbuild-101-was-passiert-wenn-ich-auf-build-klicke.html" rel="alternate" type="text/html" title="Blog post to my talk “MSBuild 101 – Was passiert wenn ich auf „Build“ klicke”" /><published>2018-02-04T00:00:00+01:00</published><updated>2018-02-04T00:00:00+01:00</updated><id>http://localhost:4000/2018/02/04/blog-post-msbuild-101-was-passiert-wenn-ich-auf-build-klicke</id><content type="html" xml:base="http://localhost:4000/2018/02/04/blog-post-msbuild-101-was-passiert-wenn-ich-auf-build-klicke.html"><![CDATA[Last Monday (30. January) a colleague and I gave a talk about the basics of MSBuild. So that also others can enjoy my explanations, I wrote this post.

All examples can be found here: 

# Classic Hello World

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <Target Name="HelloWorld">
        <Message Text="Hello World!" />
    </Target>

</Project>
```

Command: msbuild %Filename%.csproj /t:HelloWorld

That's the classic "Hello World" as for every programming language. And yes, MSBuild looks like XML, but it's a programming language. "Target" defines a "method", which can call other targets and tasks. Tasks are functions that are defined in assemblies.

With the /t parameter, you specific which target is executed. When you don't specify it, a default target is executed, but in this case, we want that the "HelloWorld" target is executed and print "Hello World!".


# Variables
As functions/methods there are also variables in MSBuild. You define them in PropertyGroups.

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <PropertyGroup>
        <HelloText>World</HelloText>
    </PropertyGroup>

    <Target Name="HelloWorld">
        <Message Text="Hello $(HelloText)!" />
    </Target>

</Project>
```

You can access them with $(%VariableName%).

# Parameters
To get some data from outside into your MSBuild script, you can pass parameters with /p and access them as other variables.

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <Target Name="HelloWorld">
        <Message Text="Hello $(HelloText)!" />
    </Target>

</Project>
```

Command: msbuild HelloParameter.csproj /t:HelloWorld /p:HelloText=Internet


# Ifs aka Conditions
And as in other languages, there are also Ifs available. Here are they called Conditions and 

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <PropertyGroup>
        <HelloText Condition="$(HelloText)==''">World</HelloText>
    </PropertyGroup>


    <Target Name="HelloWorld">
        <Message Text="Hello $(HelloText)!" />
    </Target>

</Project>
```

You can add the Condition element on nearly everything. It could be variables, whole property groups, targets or tasks.

This example uses conditions to set the variable "HelloText" to a default value if it wasn't defined via parameter.

# multiple Targets
Who wants to write everything in a single method? Nobody. But nothing stops you to write more targets.

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <Target Name="HelloWorld">
        <CallTarget Targets="PrintHello"/>
        <CallTarget Targets="PrintWorld"/>
        <CallTarget Targets="PrintExclamationMark"/>        
    </Target>

    <Target Name="PrintHello">
        <Message Text="Hello" />
    </Target>

    <Target Name="PrintWorld">
        <Message Text="World" />
    </Target>

    <Target Name="PrintExclamationMark">
        <Message Text="!" />
    </Target>

</Project>
```

With the "CallTarget" task you can execute other targets.

# "Events"
Sometimes you can not add a "CallTarget" to get your own code executed in MSBuild. For that there is a feature, that you can also trigger your targets before or after another target.

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <Target Name="HelloWorld">
        <Message Text="Hello World!" />
    </Target>


    <Target Name="Before" BeforeTargets="HelloWorld">
        <Message Text="BeforeHelloWorld" />        
    </Target>

    <Target Name="After" AfterTargets="HelloWorld">
        <Message Text="AfterHelloWorld" />        
    </Target>

</Project>
```

With setting the Before/After- Targets elements you register your target to be executed before or after the target. Real nice feature to hang your targets into the existing lifecycle of compiling a project.

# Files
Sometimes you need to execute a target for a list of files. This is possible with ItemGroups.

``` xml
<Project Sdk="Microsoft.NET.Sdk">

    <ItemGroup>
        <Files Include="*.*" />
    </ItemGroup>

    <Target Name="HelloWorld"
            Inputs="@(Files)"
            Outputs="%(Identity).Dummy">
        <Message Text="@(Files)" />
    </Target>
   
</Project>
```

With ItemGroups you can define a variable that holds a list of files. You can include and exclude to this variable with additional entries in the ItemGroup.

To get a target executed per entry in a file variable, you have to use the Inputs and Outputs attributes. In difference to variables, you have to use @(…) and not $(…).
You have to set the Outputs attribute if you set the Inputs attribute. In this case, we can set it to %(Identity).Dummy, because we generate no output files.

# The end of my part
So that was my part of the talk. My colleague Raoul Holzer continued with a little bit more advanced stuff. His examples can be found here: https://github.com/RaoulHolzer/MSBuild201 
And as soon he has written his blog post, I will link it here.]]></content><author><name></name></author><summary type="html"><![CDATA[Last Monday (30. January) a colleague and I gave a talk about the basics of MSBuild. So that also others can enjoy my explanations, I wrote this post.]]></summary></entry><entry><title type="html">1st year as a Microsoft MVP</title><link href="http://localhost:4000/2018/01/19/1st-year-microsoft-mvp.html" rel="alternate" type="text/html" title="1st year as a Microsoft MVP" /><published>2018-01-19T00:00:00+01:00</published><updated>2018-01-19T00:00:00+01:00</updated><id>http://localhost:4000/2018/01/19/1st-year-microsoft-mvp</id><content type="html" xml:base="http://localhost:4000/2018/01/19/1st-year-microsoft-mvp.html"><![CDATA[A year ago I got out of nowhere following eMail:

![mvp mail screenshot](/images/2018-01-19-MVP.png)

I was awarded as a Microsoft MVP in the "Visual Studio and Development Technologies" Category. How cool is that? And how did this happen?

I had filled out a profile and entered my activities on <https://mvp.microsoft.com/,> but the feedback that I got months before the eMail, was that a little bit is missing and it needed a some more activities to become a MVP. But as it looks now, it was still enough. 😉

So what did I do now in the last year as a MVP?

First I continued my work on SpecFlow (<http://www.specflow.org>; <https://www.github.com/techtalk/SpecFlow>) not only in my dayjob, but also in my spare time.

Motivated through the award, I wanted to give more talks and bring the .NET community in Vienna more together. Together with my colleague Raoul Holzer, we did together following talks and events in the last 12 months:

- Serverless mit Azure Functions – <https://www.meetup.com/dotnet-austria/events/237518962/>
- Neue Funtionen von VS2017 – <https://www.meetup.com/dotnet-austria/events/238491273/>
- Xamarin Dev Days 2017 – <https://www.meetup.com/Visual-Studio-User-Group-Wien/events/238287537/>
- .NET Conf Public Viewing  – <https://www.meetup.com/preview/dotnet-austria/events/243200845>
- .NET Standard – Was ist das und wozu brauche ich es? – <https://www.meetup.com/dotnet-austria/events/243847685/>﻿

And we have already planned some for the future:

- MSBuild 101 – Was passiert wenn ich auf „Build“ klicke – <https://www.meetup.com/dotnet-austria/events/246014952/>﻿
- Neues Projectsystem – was bringt mir das neue CSProj Format – <https://www.meetup.com/dotnet-austria/events/246015220/>﻿
- Building Bridges: F# for C# Developers – <https://www.meetup.com/FSharp-Vienna/events/245893317/>

Since the middle of December I am now also Co- Organizer of the .NET Community Austria meetup group and the F# |> Vienna meetup group and help Jörg and Andi with organising the meetups.

I have to thank my company TechTalk for supporting the meetups with a location, drinks and food.

Ok, that was the "active" stuff, that happened in the last year. But what happened else?

You get some really nice benefits when you are being a MVP and I really enjoy them. Some of them are from Microsoft, some are from other companies.
So you get an Office 365 subscription and a Visual Studio Subscription. From JetBrains you get a Resharper and Rider license. That makes developing really easy.

At the end of May/begin of June there was the MVP Community Connect in Madrid, which was really fun. There I had the change to meet some other MVPs from Austria, Spain, Portugal and Italy.

But the coolest benefit of being a MVP is the MVP Global Summit in Redmond where all MVPs of the world get together for some days and are able to get in touch with the product groups at Microsoft. As this will be my first Summit, I am really excited to attend it.

So if you are doing community work in the Microsoft technology stack or working on Open Source projects that has anything to do with Microsoft, go and nominate yourself for the MVP.

Hope to see you then at the next MVP Global Summit!]]></content><author><name></name></author><summary type="html"><![CDATA[A year ago I got out of nowhere following eMail:]]></summary></entry><entry><title type="html">Multiple projects in the same folder</title><link href="http://localhost:4000/2017/10/12/multiple-projects-in-same-folder.html" rel="alternate" type="text/html" title="Multiple projects in the same folder" /><published>2017-10-12T01:00:00+02:00</published><updated>2017-10-12T01:00:00+02:00</updated><id>http://localhost:4000/2017/10/12/multiple-projects-in-same-folder</id><content type="html" xml:base="http://localhost:4000/2017/10/12/multiple-projects-in-same-folder.html"><![CDATA[In SpecFlow+ we are supporting multiple different versions of SpecFlow (currently 1.9, 2.1 and 2.2) and so we have to test each version with the full set of our test suite.
As we don't want to copy a lot of code and then change some references/package version, I was looking if it is without much pain possible, that you have multiple projects/csprojs in one folder and produce so multiple assemblies.
We are already having a solution with the old project system, but it involves a lot of  linked files and manual editing of the project files. So with the new project system it was time to look at this again.

And after some work I had a solution to achieve this with the new csproj format. Here it is:

``` xml
<Project>
    <PropertyGroup>
        <BaseIntermediateOutputPath>obj\$(MSBuildProjectName)</BaseIntermediateOutputPath>
    </PropertyGroup>
    
    <Import Project="Sdk.props" Sdk="Microsoft.NET.Sdk"/>

    <PropertyGroup>
        <TargetFramework>netstandard2.0</TargetFramework>
        <OutputPath>bin\$(Configuration)\$(TargetFramework)\$(MSBuildProjectName)</OutputPath>
    </PropertyGroup>

    <Import Project="Sdk.targets" Sdk="Microsoft.NET.Sdk"/>

    <ItemGroup>
        <Compile Remove="obj\\**\*"/>
        <Compile Include="obj\$(MSBuildProjectName)\**\$(MSBuildProjectName).AssemblyInfo.cs"/>
    </ItemGroup>
</Project>
```

So what does it in detail?

``` xml
<Project>
```
Yes, there is no Sdk attribute defined. How does this work?

``` xml
<PropertyGroup>
    <BaseIntermediateOutputPath>obj\$(MSBuildProjectName)</BaseIntermediateOutputPath>
</PropertyGroup>
```

We are adjusting the BaseIntermediateOutputPath, so that each project has it's own and don't replace each others file. This is also the reason, why we have no Sdk attribute set. This property has to be set before the Sdk.props file is evaluated.

``` xml
<Import Project="Sdk.props" Sdk="Microsoft.NET.Sdk"/>
```

So, finally import the first part of the Sdk.

``` xml
<PropertyGroup>
    <TargetFramework>netstandard2.0</TargetFramework>
    <OutputPath>bin\$(Configuration)\$(TargetFramework)\$(MSBuildProjectName)</OutputPath>
</PropertyGroup>
```

The TargetFramework property is well known. Nothing to add about this here.
The OutputPath property is the location where the finished assemblies are copied. As BaseIntermediateOutputPath it has to be adjusted, so that each project has it's own folder.

``` xml
<Import Project="Sdk.targets" Sdk="Microsoft.NET.Sdk"/>
```

And now the second and last part of the Sdk is imported.

``` xml
<ItemGroup>
    <Compile Remove="obj\\**\*"/>
    <Compile Include="obj\$(MSBuildProjectName)\**\$(MSBuildProjectName).AssemblyInfo.cs"/>
</ItemGroup>
```

This is one of the stranges parts of the file. The reason for this is, that during build the version infos from the csproj are written to an *.AssemblyInfo.cs file in the obj folder. Combined with the globbing in the new format, MSBuild now finds multiple *.AssemblyInfo.cs files (one for each project).
So we have to remove the whole obj subtree from the compiler and readd the AssemblyInfo.cs for our project to get the version infos into the assembly.

Everything clear now? 😉
And the best, Visual Studio 2017 can handle this files without problems.

You can find a complete example with two projects here: <https://github.com/SabotageAndi/MultipleProjecsInSameFolder>

Conclusion: I have to say, I am very happy with the new project system. Something like this wouldn't be possible with the old one (or it would be a lot more painful).
Let's see what nice hacks are also possible in the future.]]></content><author><name></name></author><summary type="html"><![CDATA[In SpecFlow+ we are supporting multiple different versions of SpecFlow (currently 1.9, 2.1 and 2.2) and so we have to test each version with the full set of our test suite. As we don’t want to copy a lot of code and then change some references/package version, I was looking if it is without much pain possible, that you have multiple projects/csprojs in one folder and produce so multiple assemblies. We are already having a solution with the old project system, but it involves a lot of linked files and manual editing of the project files. So with the new project system it was time to look at this again.]]></summary></entry></feed>